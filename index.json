[{"uri":"http://konveyor.github.io/crane/","title":"Crane","tags":[],"description":"","content":"Crane Use this section to better understand and use the Konveyor Crane tool.\n"},{"uri":"http://konveyor.github.io/forklift/","title":"Forklift","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"http://konveyor.github.io/tackle/","title":"Tackle","tags":[],"description":"","content":"Tackle Use this section to better understand and use the Konveyor Tackle tool.\n"},{"uri":"http://konveyor.github.io/tackle/overview/","title":"Overview","tags":[],"description":"","content":"Tackle is a collection of tools that support the modernization and migration of applications to Kubernetes. These tools assess applications to determine which option is the appropriate migration strategy for each application:\n Rehosting Replatforming Refactoring  Tackle uses an interactive questionnaire for the assessment which enables key stakeholders to gather information about applications, discuss risks flagged by Tackle, and reach a consensus in formulating recommendations for each application.\nTackle Refactoring Tools The tools are cloud-native micro-services that are accessible from a common Tackle UI.\n Application Inventory Pathfinder Controls  Application Inventory Tackle Application Inventory is the vehicle which selects applications for assessment by Pathfinder. It provides users four main functions:\n Maintain a portfolio of applications. Link applications to the business services they support. Define interdependencies. Add metadata using an extensible tagging model to describe and categorize applications in multiple dimensions.  Pathfinder Tackle Pathfinder is an interactive questionnaire based tool that assesses the suitability of applications for modernization in order to deploy them into containers on an enterprise Kubernetes platform. The tool output includes::\n Application’s suitability for Kubernetes Associated risks Adoption plan with the applications’ prioritization, business criticality and dependencies.  Controls Tackle Controls are a collection of entities that add value to Application Inventory and the Pathfinder assessment. They comprise business services, stakeholders, stakeholder groups, job functions, tag types, and tags.\nTackle Controls are a collection of entities that add value to the Application Inventory and the Pathfinder assessment including:\n Business services Stakeholders Stakeholder groups Job functions Tag types Tag  Tackle Projects  Tackle Web UI Tackle Application Inventory Tackle Pathfinder Tackle Controls Tackle Documentation Tackle Commons REST Tackle Keycloak Theme Tackle DiVA Tackle Test Generator Tackle Container Advisor  "},{"uri":"http://konveyor.github.io/tackle/upgrade/","title":"Upgrading Tackle","tags":[],"description":"","content":"Tackle application instances are upgraded manually.\nUpgrading from version 1.1.0 to 1.2.0 Follow the steps below to manually upgrade an instance of the Tackle application from 1.1.0 to 1.2.0.\nPrerequisites\nProject administrator privileges.\nProcedure\nSpecify the namespace and the Tackle instance name for each step.\n Update the Keycloak deployment of the Tackle instance:  $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-keycloak keycloak-theme=quay.io/konveyor/tackle-keycloak-init:1.2.0 Update the application-inventory-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-application-inventory-rest \\ \u0026lt;tackle_instance\u0026gt;-application-inventory-rest=quay.io/konveyor/tackle-application-inventory:1.2.0-native Update the controls-rest deployment:  $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-controls-rest \\ \u0026lt;tackle_instance\u0026gt;-controls-rest=quay.io/konveyor/tackle-controls:1.2.0-native Update the pathfinder-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-pathfinder-rest \\ \u0026lt;tackle_instance\u0026gt;-pathfinder-rest=quay.io/konveyor/tackle-pathfinder:1.2.0-native Update the UI deployment:  $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-ui \\ \u0026lt;tackle_instance\u0026gt;-ui=quay.io/konveyor/tackle-ui:1.2.0 Log in to the web console and click the Help icon beside the user name to verify the upgrade.  The About Tackle window opens and displays the version number.\nUpgrading from version 1.0.0 to 1.1.0 Follow the steps below to manually upgrade an instance of the Tackle application from 1.0.0 to 1.1.0.\nPrerequisites\nProject administrator privileges.\nProcedure\nSpecify the namespace and the Tackle instance name for each step.\n Update the Keycloak deployment of the Tackle instance:  $ kubectl -n \u0026lt;namespace\u0026gt; exec deployments/\u0026lt;tackle_instance\u0026gt;-keycloak \\ -c \u0026lt;tackle_instance\u0026gt;-keycloak -- bash -c \u0026#39;/opt/jboss/keycloak/bin/kcadm.sh \\ update realms/tackle -s internationalizationEnabled=true -s supportedLocales+=en \\ -s supportedLocales+=es -s defaultLocale=en --server http://localhost:8080/auth \\ --realm master --user $KEYCLOAK_USER --password $KEYCLOAK_PASSWORD\u0026#39; Update the application-inventory-rest deployment:  $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-application-inventory-rest \\ \u0026lt;tackle_instance\u0026gt;-application-inventory-rest=quay.io/konveyor/tackle-application-inventory:1.1.0-native Update the controls-rest deployment:  $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-controls-rest \\ \u0026lt;tackle_instance\u0026gt;-controls-rest=quay.io/konveyor/tackle-controls:1.1.0-native Update the pathfinder-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-pathfinder-rest \\ \u0026lt;tackle_instance\u0026gt;-pathfinder-rest=quay.io/konveyor/tackle-pathfinder:1.1.0-native 4.Update the UI deployment:\n$ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-ui \\ \u0026lt;tackle_instance\u0026gt;-ui=quay.io/konveyor/tackle-ui:1.1.0 Log in to the web console and click the Help icon beside the user name to verify the upgrade.  The About Tackle window opens and displays the version number.\n"},{"uri":"http://konveyor.github.io/tackle/webconsolesvcs/","title":"Web Console Services","tags":[],"description":"","content":"Tackle web console provides the following services:\n Application inventory Assessments Reviews Reports Controls  Application inventory The Application inventory page enables you to perform the following tasks:\n Manage your application portfolio. Define and manage application dependencies. Link applications to the business services that they support. Describe and categorize applications by using tags.  Assessments The applications are assessed on the Application inventory page which helps determine the appropriate migration strategy for each application:\n Rehosting Replatforming Refactoring  Reviews Review a completed assessment on the Application inventory page where the following information is collected from each reviewer:\n Proposed action: Best alternative for modernizing and migrating the application. Effort estimate: The likely amount of effort involved in migrating the application based on the proposed action and the identified risks. Business criticality: Estimate of how critical the application is to your business on a scale of 1 to 10. Work priority: Estimated priority for the proposed action on a scale of 1 to 10.  The Reports page displays the results of the reviews.\nReports The Reports page displays the results of the application assessments and reviews and can help you to plan your migration by providing the following information:\n Suitability of applications for containerization. Possible risks and severity. Adoption plan based on estimated effort, work priority, and application dependencies.  Controls The Controls page enables you to manage the following entities:\n Stakeholder: Individual with an interest in an application or a subject matter expert. Stakeholder group: Related stakeholders who may belong to one or more stakeholder groups used to assign multiple stakeholders to review an assessment. Job function: Attribute of a stakeholder using a predefined job function or by creating a new job function. Business service: Attribute of an application, for example, credit card service, transportation, or IT support using a predefined business service or by creating a new business service. Tag: Attribute of an application that are an extensible and flexible way to add metadata to applications. Tags are grouped under a parent tag type using a predefined tag or by creating a new tag. Tag type: Defines the rank and background color of a group of related tags.  "},{"uri":"http://konveyor.github.io/tackle/manageusers/","title":"Managing users and credentials","tags":[],"description":"","content":"Follow the procedures in this section to manage Tackle users and passwords using the Keycloak admin console.\nAccessing the Keycloak admin console A Keycloak admin user is created when Tackle is installed. The admin username and password are needed to log in to the Keycloak admin console. The credentials are stored in the tackle-keycloak secret.\nPrerequisites Cluster-admin privileges.\nProcedure  Run the following command to obtain the admin credentials:  $ kubectl get secret tackle-keycloak -o go-template=\u0026#39;{{range $k,$v := .data}}{{printf \u0026#34;%s: \u0026#34; $k}}{{if not $v}}{{$v}}{{else}}{{$v | base64decode}}{{end}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; Example output ADMIN_PASSWORD: \u0026lt;password\u0026gt; ADMIN_USERNAME: admin Launch a browser and navigate to https://\u0026lt;www.example.com\u0026gt;/auth and specify the Tackle cluster URL. Log in to the Keycloak admin console with the admin user name and password.  Changing the default password Follow the steps below to change the default password of the tackle user.\nPrerequisites  Cluster-admin privileges Keycloak admin user name and password.  Procedure  Log in to the Keycloak admin console at https://\u0026lt;www.example.com\u0026gt;/auth and specify the Tackle cluster URL. Locate the tackle user in the Tackle realm. See Searching for users in the Keycloak documentation. Update the tackle user’s password. See User Credentials. Log out of the Keycloak admin console. Log in to the Tackle web console as the tackle user to verify the new password.  Adding users Follow the steps below to add users by using the Keycloak admin console.\nPrerequisites  Cluster-admin privileges Keycloak admin user name and password.  Procedure  Log in to the Keycloak admin console at https://\u0026lt;www.example.com\u0026gt;/auth and specify the Tackle cluster URL. Create a new user in the Tackle realm. See Creating a new user in the Keycloak documentation. Create a password for the new user. See Creating a password for the user.  Optional: Set attributes and permissions for the new user. See User profile.\nAdditional resources for Keycloak  Admin console in the Keycloak documentation. User management in the Keycloak documentation. Keycloak Operator on Kubernetes in the Keycloak documentation.  "},{"uri":"http://konveyor.github.io/tackle/manageassess/","title":"Managing assessments","tags":[],"description":"","content":"Start, edit, review, and delete application assessments in the Application inventory page of the Tackle web console.\nStarting an assessment Follow the steps below to start an application assessment on the Application inventory page of the Tackle web console.\nProcedure  Start the Tackle web console Click Application Inventory. Select an application that does not have a Completed assessment status and click the Assess button in the toolbar. Select individual stakeholders or stakeholder groups and then click Next. Select responses to all questions on each page of the Application Assessment wizard and then click Next.  Important: All questions are mandatory.\nClick Save to save the assessment, or Save and Review to start the assessment review process.  The Assessment status of each assessed application is set to Completed.\nEditing an assessment Follow the steps below to take an application assessment on the Application inventory page of the Tackle web console.\nPrerequisites An application must have a Completed assessment status.\nProcedure  Open the Tackle web console. Click Application Inventory. Select an application that does not have a Completed assessment status and click the Assess button in the toolbar. Click Continue to confirm you want to edit the assessment. Update entries. Click Save to save the assessment, or Save and Review to start the assessment review process.  ##Reviewing an assessment Follow the steps below to review an application assessment on the Application inventory page of the Tackle web console.\nPrerequisites An application must have a Completed assessment status and a Not started review status.\nProcedure  Open the Tackle web console. Click Application Inventory. Select an application with a Completed assessment status and a Not Started review status and click the Review button in the toolbar. Review the assessment in the Assessment summary section on the Review page.  Note: The Assessment summary table contains a Risk column that indicates the severity of the risk associated with each response.\nSelect a Proposed action and an Effort estimate. Set values for Business criticality and Work priority.  Recommended: Enter comments in the Comments field.\nClick Submit Review.  The Review status of each application is set to Completed.\nExpand the application to view the review results.  Copying and applying assessments and reviews. Follow the steps below to:\n Copy an assessment or assessment and review from a single application and apply them to multiple applications on the Application inventory page of the Tackle web console. Apply assessments and reviews to groups of related applications, for example, applications written in Java or belonging to the same business service.  Prerequisites An application must have a Completed assessment status or Completed assessment and review statuses, depending on whether you are copying an assessment or an assessment and review.\nProcedure  Open the Tackle web console. Click Application Inventory. Click the Options menu kebab beside an application with a Completed assessment status or Completed assessment and Review statuses. Select Copy assessment or Copy assessment and review.  Optional: In the dialog box, click Name to select a filter, for example:\n Tag and select a tag. Java to display a filtered list of applications.  Select the applications to which you want to apply the copied assessment or assessment and review. If a selected application has an existing assessment or review, select the Yes, continue check box to confirm that existing assessments and reviews will be overwritten. Click Copy.  The selected applications are set to a Completed assessment status or Completed assessment and review statuses.\nDeleting an assessment Follow the steps below to delete an application assessment on the Application inventory page of the Tackle web console. Deleting an assessment deletes its review.\nPrerequisites An application must have a Completed assessment status.\nProcedure  Open the Tackle web console. Click Application Inventory. Click he Options menu kebab beside an application with a Completed assessment status and select Discard assessment. Click Continue to confirm the deletion.  The Assessment and Review status of the application are set to Not started.\n"},{"uri":"http://konveyor.github.io/tackle/manageapps/","title":"Managing applications","tags":[],"description":"","content":" title: \u0026ldquo;Managing Applications\u0026rdquo; date: 2022-05-04T17:02:46-06:00 draft: true Follow the procedures in this section to create, import, tag, and modify your applications in the Application inventory page of the Tackle web console.\nCreating an application Follow the steps below to create an application on the Application inventory page of the Tackle web console.\nProcedure  Open the Tackle web console. Click Application inventory and then Create New. Complete the following fields:   Name: Name of the application. Description: Optional. Description of the application. Business service: Optional. You can select a business service that describes the application. Tags: Optional. You can select one or more tags. Comments: Optional. Comments about the application.  Click Create.  The new application is displayed on the Application inventory page. Expand the application to view its tags and comments.\nImporting applications Follow the steps below to import one or more applications into the Application inventory page of the Tackle web console by using a CSV file.\nNote: You cannot create tags or business services by importing a CSV file. Specified tags or business services must exist in the web console before you import the applications.\nThe CSV file contains the following fields:\n Record Type 1: Describes an application or application dependencies. Required for all records.  The following values are allowed for Record Type 1:\n Application: This option has the following fields:  Application Name: Required Description: Optional Comments: Optional Business Service: Optional. Must exist in the web console Tag Type \u0026lt;1..20\u0026gt;: Optional. Must exist in the web console. Tag \u0026lt;1..20\u0026gt;: Optional. Must exist in the web console.    Note: You can import up to 20 Tag Type and Tag fields.\n Application dependencies: This option requires the following fields:  Application Name Dependency: Must be the same as the Application Name of the dependency. Dependency Direction: Allowed values are northbound and southbound.    All other fields are empty. See the CSV example below.\n   Record Type 1 Application Name Dependency Dependency Direction Description Comments Business Service Tag Type 1 Tag 1 Tag Type 2 Tag 2 Tag Type 3 Tag 3     1 Imported-Purchasing Vendor management Required for purchase order processing and accounts payable Finance and HR Operating System Z/OS Database DB2 Language COBOL     2 Imported-Purchasing Tiller northbound            1 Imported-PO   Requisitions, purchase orders, goods received Requisition to receipt Finance and HR Operating System Z/OS Database DB2 Language COBOL   2 Imported-Payroll Imported-GL northbound            2 Imported-Payroll Imported-HR southbound            1 Imported-GL   General Ledger  Finance and HR Operating System Z/OS Database DB2 Language COBOL   1 Imported-HR   Human Resources Go live scheduled for Q3 Finance and HR Operating System RHEL 8 Database PostgreSQL Language Python    Prerequisites Valid CSV file.\nSpecified business services, tag types, and tags created in the web console.\nProcedure  Open the Tackle web console. Click Application Inventory. Click the Options menu kebab in the toolbar and select Import. Browse to the CSV file and click Open. Click Import.  The imported applications are displayed on the Application inventory page.\nManaging application imports Follow the steps below to manage application imports on the Application import page of the Tackle web console.\nProcedure  Open the Tackle web console. Click Application Inventory. Click the Options menu kebab in the toolbar and select Manage Imports.  The Application import page displays a list of application imports.\nClick the Options menu kebab beside an application import and select one of the following options:   Delete: Deletes the application import. View Error: Report displays a table of application import errors. Export Errors: Enables you to save the application import errors as a CSV file.  Updating the tags assigned to an application Follow the steps below to add or remove tags assigned to an application on the Application inventory page of the Tackle web console.\nProcedure  Open the Tackle web console. Click Application Inventory. Click the Edit icon beside an application. Add or delete tags and click Save. Expand the application to view the updated tags.  Updating the business service assigned to an application Follow the steps below to update the business service assigned to an application on the Application inventory page of the Tackle web console.\nPrerequisites The business service must exist on the Controls page.\nProcedure  Open the Tackle web console. click Application Inventory. Click the Edit icon beside an application. Select a business service and click Save.  The updated business service is displayed in the Business service column of the application.\nManaging application dependencies Follow the steps below to add, delete, and view application dependencies in the Manage dependencies window on the Application inventory page of the Tackle web console.\nProcedure  Open the Tackle web console. Click Application Inventory. Click the Options menu kebab beside an application and select Manage Dependencies. Do one of the following:   To add dependencies: Select applications in the northbound or southbound dependencies fields. To remove dependencies: Delete selected applications in the northbound or southbound dependencies fields.  Click Close to save and close.  "},{"uri":"http://konveyor.github.io/tackle/additionaltools/","title":"Additional tools","tags":[],"description":"","content":"DIVA Tackle DiVA is a data-centric application analysis tool that imports a set of target application source files and provides database/transaction analysis results.\nTest Generator Tackle Test Generator is an automatic test-generation and differential-testing tool that currently supports unit-level test generation for Java applications. (Future project plans include adding capabilities for automated generation of end-to-end UI/UX test cases for web applications and test cases for REST APIs.)\nContainer Advisor Tackle Container Advisor provides containerization advisory information for a large scale application portfolio. It takes a natural language description of applications and recommends whether the applications can be containerized in terms of images from multiple container catalogs (DockerHub, Openshift). (Future project plans include supporting Operators and recommending disposition that is explainable.)\n"},{"uri":"http://konveyor.github.io/tackle/installation/","title":"Installing Tackle","tags":[],"description":"","content":"Follow the procedures in this section to install Tackle.\nInstalling the Tackle Operator Follow the steps below to download and install the Tackle Operator on an Enterprise Kubernetes Platform cluster.\nPrerequisites  Cluster-admin privileges.  Procedure  Install the Tackle Operator:  $ kubectl create -f https://operatorhub.io/install/tackle-operator.yaml The Tackle Operator is installed in the my-tackle-operator namespace by default.\nVerify the Tackle Operator installation by viewing its resources:  $ kubectl get all -n my-tackle-operator Installing the Tackle application Follow the steps below to install Tackle in a namespace by creating an instance of the Tackle application.\nPrerequisites  Tackle Operator installed on the cluster. Project-admin privileges.  Procedure  Create an instance of the Tackle application, specifying its namespace:  $ kubectl apply -n \u0026lt;namespace\u0026gt; -f https://raw.githubusercontent.com/konveyor/tackle-operator/main/src/main/resources/k8s/tackle/tackle.yaml Note: Multiple instances of the Tackle application can be created in the same namespace by specifying a unique name for each instance in the tackle.yaml file.\nOpen the Kubernetes dashboard Click Workloads then Deployments to verify the installation.  Logging into the Tackle web console Follow the steps below to log into the Tackle web console.\nPrerequisites  Tackle application installed.  Procedure  Open the Kubernetes dashboard. Click Services then Ingresses. Click the Endpoint of the tackle-sample ingress to launch the Tackle web console in a new browser window. Enter the following:   tackle in the Username or Email field Set a password in the Password field.  Click Log in.  Important: Change the default password of the tackle user.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step1export/","title":"Step One: Export","tags":[],"description":"","content":"The first step of the cluster migration process is exporting resources from a source cluster of any namespace to be input for the subsequent commands.\nAll of the following export commands will output the contents of the foo namespace into a local export directory with the context demo defined in KUBECONFIG.\ncrane export -n foo -e export --context demo cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: demo EOF crane export -c conf.yaml cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: testing EOF crane export -c conf.yaml --context demo # Note the difference is we are overriding \u0026#34;context\u0026#34; here with flag Note: There are multiple ways to input a command, precedence of which is input from flags \u0026gt; input from config file \u0026gt; env variables \u0026gt; default values (not all the flags can have a corresponding env variable). This behavior persists across all Crane CLI commands.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step2transform/","title":"Step Two: Transform","tags":[],"description":"","content":"The Transform command facilitates the changes to the exported resources that are frequently necessary when migrating workloads between one environment to another. For example:\n Stripping the resource status information that is no longer relevant after the resource is serialized out of a cluster. Adjusting resource quotas to fit the destination environment. Altering node selectors to match the new environment if the node labels do not match the source environment. Applying custom labels or annotations to resources during the migration.  Because most changes are specific to an environment, Crane is designed for total customization and the transform command accepts a “plugins” directory argument.\nEach plugin is an executable with a well defined stdin/out interface allowing for customization or installation and use of published generic plugins.\nAfter exporting the resources into a local directory and installing the desired transformation plugins, the crane transform command can run. The output is placed in a directory with a set of transform files that describe the changes that need to be applied to the original resources before their final import. The changes are written in the JSONPatch format, are human readable, and easily hackable.\nThis command generates a patch to add an annotation transform-test:test for objects in the export directory and the transform directory to be used as input for apply command.\ncrane transform -e export -p plugins -t transform --optional-flags=\u0026#34;add-annotations=transform-test:test\u0026#34; Run the following to see the list of available optional commands for configured plugins.\ncrane transform optionals See Managing PlugIns for more information on plugins that can be consumed by the transform command.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step3apply/","title":"Step Three: Apply","tags":[],"description":"","content":"The final step of the cluster migration process is to apply all the patches generated by the Transform command to exported resources.\ncrane apply -e export -t transform -o output Apply the patches in the transform directory to the resources in the export directory and save the modified resource files in the output directory.\nAfter applying the patches, the resources located in output directory can either be deployed to the destination cluster using kubectl apply, or they can be pushed to a repository and then applied with the help of the GitOps pipeline. An example of the later scenario can be found here.\n"},{"uri":"http://konveyor.github.io/crane/overview/","title":"Crane overview","tags":[],"description":"","content":"Crane is a tool that helps application owners migrate Kubernetes workloads and their state between clusters, remove environment-specific configuration, and automate application deployments along the way.\nThe process uses a few tools:\n crane: The command line tool that migrates applications to the terminal. crane-lib: The brains behind Crane functionality responsible for transforming resources. crane-plugin-openshift: Plugin specifically tailored to manage OpenShift migration workloads and an example of a repeatable best-practice. crane-plugins: Collection of plugins from the Konveyor community based on experience from performing Kube migrations.  Why crane is needed? Crane is the product of several years of experience performing large-scale production Kubernetes migrations that are large, complex, error-prone, and usually peformed in a limited window of time.\nTo face those challenges, the Crane migration tool is designed with transparency and ease-of-diagnostics in mind. It drives migration through a pipeline of non-destructive tasks that output results to disk so the operation can be easily audited and versioned without ever impacting live workloads. The tasks can be run repeatedly and will output consistent results given the same inputs without side-effects on the system at large.\n"},{"uri":"http://konveyor.github.io/crane/tutorials/","title":"Tutorials","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/crane/tools/","title":"Tools","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/crane/usingcrane/","title":"Using Crane","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/crane/tools/gitopsintegration/","title":"Integrating GitOps","tags":[],"description":"","content":"All Crane commands are individual utilities, but when used together in sequence, they form a pipeline.\nCrane makes it easy to integrate a gitops that applies the patches/resources generated at the end of the apply command on the destination cluster. The resources generated at the end of the process (i.e export, transform, apply) can be pushed to a github repository, and a pipeline can be created to deploy the resources on a cluster on every push.\n"},{"uri":"http://konveyor.github.io/crane/tools/customplugins/","title":"Developing custom plugins","tags":[],"description":"","content":"This document covers how to write a plugin binary using crane-lib. It requires:\n  Go to the development environment setup. (Optionally, an overview of the crane toolkit.)\n  Create binary plugin for crane-lib as a simple Go program in the following format that will:\n   Read an input from stdin. Call the Run function with the input object passed as unstructured. Print the return value of Run function on stdout.  package main import ( \u0026#34;fmt\u0026#34; jsonpatch \u0026#34;github.com/evanphx/json-patch\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform/cli\u0026#34; ) func main() { fields := []transform.OptionalFields{ { FlagName: \u0026#34;my-flag\u0026#34;, Help: \u0026#34;What the flag does\u0026#34;, Example: \u0026#34;true\u0026#34;, }, } cli.RunAndExit(cli.NewCustomPlugin(\u0026#34;MyCustomPlugin\u0026#34;, \u0026#34;v1\u0026#34;, fields, Run)) } func Run(request transform.PluginRequest) (transform.PluginResponse, error) { // plugin writers need to write custom code here. resp := transform.PluginResponse{ Version: string(transform.V1), } // prepare the response return resp, nil } The json is passed in using stdin is a transform.PluginRequest which consists of an inline unstructured object and an optional Extras map containing additional flags. Without any Extras the format is identical to the json output from a kubectl get -o json call.\nWhen adding extra parameters, a map field “extras” is added at the top level (parallel to “apiVersion”, “kind”, etc.).\nVersion the plugin development output by passing in the JSOC object on stdin manually during development. For example, if the code above is compiled and run with the following json as input, the output will be {\u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot;}.\n./my-plugin { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Route\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;openshift.io/host.generated\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:spec\u0026#34;: { \u0026#34;f:path\u0026#34;: {}, \u0026#34;f:to\u0026#34;: { \u0026#34;f:kind\u0026#34;: {}, \u0026#34;f:name\u0026#34;: {}, \u0026#34;f:weight\u0026#34;: {} }, \u0026#34;f:wildcardPolicy\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;oc\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:ingress\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;openshift-router\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;mssql-app-route\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;mssql-persistent\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;155816271\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/route.openshift.io/v1/namespaces/mssql-persistent/routes/mssql-app-route\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;42dca205-31bf-463d-b516-f84064523c2c\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;to\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mssql-app-service\u0026#34;, \u0026#34;weight\u0026#34;: 100 }, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;ingress\u0026#34;: [ { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Admitted\u0026#34; } ], \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerCanonicalHostname\u0026#34;: \u0026#34;apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; } ] } } When the plugin is ready to be tested, put it in a directory and run with the crane cli command.\nMore accurate detail can be found [here] (https://github.com/konveyor/crane-lib/blob/main/transform/binary-plugin/README.md).\n"},{"uri":"http://konveyor.github.io/crane/tools/tunnelapi/","title":"Tunnel API","tags":[],"description":"","content":"The tunnel-api sub-command can be used to access an on-premise cluster from a cloud cluster to allow orchestrating migrations from on-premise clusters using MTC where access is not possible otherwise.\nAn openvpn client on the on-premise cluster will connect to a server running on the cloud cluster and the openvpn server is exposed to the client using a load balancer address on the cloud cluster.\nA service created on the cloud cluster is used to expose the on-premise clusters API to MTC running on the cloud cluster.\nRequirements  The system used to create the VPN tunnel must have access and be logged in to both clusters. It must be possible to create a load balancer on the cloud cluster. An available namespace on each cluster to run the tunnel in not created in advance.  Note: To connect multiple on-premise source clusters to your cloud cluster use a separate namespace for each.\napi-tunnel options  namespace: The namespace used to launch the VPN tunnel in, defaults to openvpn destination-context: The cloud destination cluster context where the openvpn server will be launched. destination-image: The container image to use on the destination cluster. (Default: quay.io/konveyor/openvpn:latest) source-context: The on-premise source cluster context where the openvpn client will be launched. source-image: The container image to use on the source cluster. (Default: quay.io/konveyor/openvpn:latest) proxy-host: The hostname of an http-proxy to use on the source cluster for connecting to the destination cluster. proxy-pass: The password for the http-proxy. If specified you must also specify a username or it will be ignored. proxy-port: The port the http-proxy is listening on. If none is specified it will default to 3128 proxy-user: The username for the http-proxy. If specified you must also specify a password or it will be ignored.  Example\ncrane tunnel-api --namespace openvpn-311 \\ --destination-context openshift-migration/c131-e-us-east-containers-cloud-ibm-com/admin \\ --source-context default/192-168-122-171-nip-io:8443/admin \\ --source-image: my.registry.server:5000/konveyor/openvpn:latest \\ --proxy-host my.proxy.server \\ --proxy-port 3128 \\ --proxy-user foo \\ --proxy-pass bar MTC Configuration When configuring the source cluster in MTC the API URL takes the form of https://proxied-cluster.${namespace}.svc.cluster.local:8443.\nYou may also set the image registry for direct image migrations to proxied-cluster.${namespace}.svc.cluster.local:5000.\nReplace ${namespace} with either openvpn or the namespace you specified when running the command to set up the tunnel.\nDemo https://youtu.be/wrPVcZ4bP1M\nTroubleshooting It may take 3 to 5 minutes after the setup to complete for the load balancer address to become resolvable. During this time the client will be unable to connect and establish a connection and the tunnel will not function.\nDuring this time you can run oc get pods in the namespace you specified for setup, and monitor the logs of the openvpn container to see the connection establish.\nExample\noc logs -f -n openvpn-311 openvpn-7b66f65d48-79dbs -c openvpn "},{"uri":"http://konveyor.github.io/crane/tools/pluginmanager/","title":"Plugin Manager","tags":[],"description":"","content":"The Plugin Manager is an optional utility that assists in adding plugins to the appropriate location to be consumed by the transform command.\nList Plugin utility The List Plugin utility discovers available plugins that that are compatible with the current OS and architecture.\ncrane plugin-manager list Listing from the repo default +-----------------+------------------+-------------------+ | NAME | SHORTDESCRIPTION | AVAILABLEVERSIONS | +-----------------+------------------+-------------------+ | OpenshiftPlugin | OpenshiftPlugin | v0.0.1 | +-----------------+------------------+-------------------+ Other valid execution examples This command lists all installed plugins managed by plugin-manager.\ncrane plugin-manager --installed -p plugin-dir This command lists all version of the foo plugin with detailed information.\ncrane plugin-manager --params -n foo Add Plugin utility The Add Plugin utility places the plugin into a directory to be consumed by Transform command.\nThis command downloads the binary of the foo version 0.0.1 plugin from the appropriate source and places it in the plugin-dir/managed directory (the default is plugins).\ncrane plugin-manager add foo --version 0.0.1 -p plugin-dir Remove Plugin utility The Remove Plugin utility removes unwanted plugins from being consumed by the Transform command. This command removes the foo plugin from the plugin-dir/managed dir.\ncrane plugin-manager remove foo -p plugin-dir Note: The plugin-manager command operates in the \u0026lt;plugin-dir\u0026gt;/managed directory. Whenever the flag -p, plugin-dir is used with plugin-manager, the utility operates in the managed places folder in \u0026lt;plugin-dir\u0026gt;.\nFor example: plugin-manager add places the plugin binary within \u0026lt;plugin-dir\u0026gt;/managed, plugin-manager removes the binary from \u0026lt;plugin-dir\u0026gt;/managed, and plugin-manager list --installed uses the path \u0026lt;plugin-dir\u0026gt;/managed to list installed plugins.\nManual plugin management Currently only two plugins are available with more plugins available soon.\nAvailable plugins: -Kubernetes (build into crane-lib) -OpenShift.\nThese plugins can be added to the desired plugin directory. (The default directory is plugin where crane is installed.)\nImportant: The Kubernetes plugin is built into the crane-lib and is not to be added manually or otherwise.\nTo install the plugins:\nDownload the binary of the plugin from the release and place it in the plugin directory.\ncurl -sL https://api.github.com/repos/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Build the binary locally and place it in theplugin directory.\ncd $GOPATH git clone https://github.com/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;.git cd crane-plugin-\u0026lt;plugin-name\u0026gt; go build -f \u0026lt;plugin-name\u0026gt; . cp \u0026lt;plugin\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Note: Adding plugins available in the plugin repo manually is not advisable as long as it can be added usingplugin-manager. For custom plugins or testing plugins under development, manual management is necessary.\n"},{"uri":"http://konveyor.github.io/crane/tutorials/statelessappmirror/","title":"Tutorial: Stateless application mirror","tags":[],"description":"","content":"This tutorial demonstrates how to mirror a simple, stateless PHP Guestbook application using Crane.\nRoadmap\n 1. Deploy the Guestbook application in the source cluster. 2. Extract resources from the source cluster using Crane Export. 3. Transform resources to prepare manifests for the destination cluster using Crane Transform. 4. Apply the transformations using Crane Apply.  Apply application manifests to the destination cluster.    Prerequisites  Create a source and destination Kubernetes cluster environment in minikube or Kind:  minikube\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane/main/hack/minikube-clusters-start.sh\u0026#34; chmod +x minikube-clusters-start.sh./minikube-clusters-start.sh Kind\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane-runner/main/hack/kind-up.sh\u0026#34; chmod +x kind-up.sh./kind-up.sh Install Crane using the Installation Guide.  Important: Read through Kubernetes\u0026rsquo; documentation on accessing multiple clusters. This document references src and dest contexts that refer to the clusters created using the minikube startup scripts above.\nIf you are working in your own environment, or use kind (kind-src and kind-dest), you will need to modify the commands below to reference the correct cluster context.\n1. Deploy the Guestbook application in the source cluster Deploy the Kubernetes\u0026rsquo; stateless guestbook application and modify it to be consumable with Kustomize (Kubernetes native and template-free tool to manage application configuration). The guestbook application consists of:\n redis leader deployment and service redis follower deployment and service guestbook front-end deployment and service  kubectl --context src create namespace guestbook kubectl --context src --namespace guestbook apply -k github.com/konveyor/crane-runner/examples/resources/guestbook kubectl --context src --namespace guestbook wait --for=condition=ready pod --selector=app=guestbook --timeout=180s Optional\nForward localhost traffic to the frontend of the Guestbook application to access Guestbook from the browser of your choice using localhost:8080:\nkubectl --context src --namespace guestbook port-forward svc/frontend 8080:80 2. Extract from the source cluster Crane’s export command is how you extract all of the resources you want from the “source” cluster.\ncrane export --context src --namespace guestbook Check the export directory to verify it is working correctly. The directory should look similar to the example below:\n$ tree -a export export ├── failures │ └── guestbook └── resources └── guestbook ├── ConfigMap_guestbook_kube-root-ca.crt.yaml ├── Deployment_guestbook_frontend.yaml ├── Deployment_guestbook_redis-master.yaml ├── Deployment_guestbook_redis-slave.yaml ├── Endpoints_guestbook_frontend.yaml ├── Endpoints_guestbook_redis-master.yaml ├── Endpoints_guestbook_redis-slave.yaml ├── EndpointSlice_guestbook_frontend-bkqbs.yaml ├── EndpointSlice_guestbook_redis-master-hxr5k.yaml ├── EndpointSlice_guestbook_redis-slave-8wt7z.yaml ├── Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml ├── Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml ├── Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml ├── Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml ├── Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml ├── Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml ├── ReplicaSet_guestbook_frontend-5fd859dcf6.yaml ├── ReplicaSet_guestbook_redis-master-55d9747c6c.yaml ├── ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml ├── Secret_guestbook_default-token-5vsrb.yaml ├── ServiceAccount_guestbook_default.yaml ├── Service_guestbook_frontend.yaml ├── Service_guestbook_redis-master.yaml └── Service_guestbook_redis-slave.yaml 4 directories, 24 files Crane Export is using a discovery client to see all of the API resources in the specified namespace of the designated cluster and outputing them to the disk in YAML form. This allows you to migrate your workloads in a non-destructive way.\nGoing forward you’ll be working against these manifests on the disk without impacting the active resources in the “source” cluster.\n3. Generate Transformations Crane’s transform command generates tranformations in the form of JSON patches and stores them on the disk in the transform directory (unless overridden using ``\u0026ndash;transform-dir`).\ncrane transform Check the transform directory to verify the command worked correctly:\n$ tree -a transform The directory should look similar to the example below:\ntransform └── resources └── guestbook ├── transform-ConfigMap_guestbook_kube-root-ca.crt.yaml ├── transform-Deployment_guestbook_frontend.yaml ├── transform-Deployment_guestbook_redis-master.yaml ├── transform-Deployment_guestbook_redis-slave.yaml ├── transform-Secret_guestbook_default-token-5vsrb.yaml ├── transform-ServiceAccount_guestbook_default.yaml ├── transform-Service_guestbook_frontend.yaml ├── transform-Service_guestbook_redis-master.yaml ├── transform-Service_guestbook_redis-slave.yaml ├── .wh.Endpoints_guestbook_frontend.yaml ├── .wh.Endpoints_guestbook_redis-master.yaml ├── .wh.Endpoints_guestbook_redis-slave.yaml ├── .wh.EndpointSlice_guestbook_frontend-bkqbs.yaml ├── .wh.EndpointSlice_guestbook_redis-master-hxr5k.yaml ├── .wh.EndpointSlice_guestbook_redis-slave-8wt7z.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml ├── .wh.Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml ├── .wh.Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml ├── .wh.Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml ├── .wh.ReplicaSet_guestbook_frontend-5fd859dcf6.yaml ├── .wh.ReplicaSet_guestbook_redis-master-55d9747c6c.yaml └── .wh.ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml 2 directories, 24 files Crane Transform is iterating through the configured plugins and running them against the exported resources from the previous step. You can see which plugins are configured with Crane Transform list-plugins and optional arguments to those plugins with crane transform optionals.\nExplore what plugins can be configured with the Crane plugin-manager list, install one, and customize the exported resources:\ncrane transform --optional-flags=\u0026#34;add-annotations=custom-crane-annotation=foo\u0026#34; If the flags get difficult to manage via the command-line, specify a --flags-file similar to the example below::\ndebug: false export-dir: myExport transform-dir: myTransform output-dir: myOutput optional-flags: add-annotations: custom-crane-annotation: \u0026#34;foo\u0026#34; 4. Apply Transformations The Crane Apply command takes the exported resources and transformations and renders the results as YAML files that can be applied to another cluster.\ncrane apply Look at one of the transformations created in the last step to better understand the Apply command.\n$ cat transform/resources/guestbook/transform-Deployment_guestbook_frontend.yaml [{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/uid\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/resourceVersion\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/creationTimestamp\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/generation\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/status\u0026#34;}]% When crane applies the transformations for the Guestbook frontend it executes a handful of JSON patches that:\n Remove the UID Remove the resourceVersion Remove the creationTimestamp Remove the generation field Remove the status  The leftover data from the source cluster is removed from the final manifests to make them applicable to the destination cluster.\nThe resources are effectively cluster agnostic and ready to be kubectl applied to the cluster of your choosing or placed under version control to be later managed by GitOps and CI/CD pipelines.\nNote: Additional patches to add/remove/replace additional fields on the resources previously exported are available if optional flags are specified..\nApply the manifests to the destination cluster Apply the manifests prepared for the destination cluster using kubectl directly:\nkubectl --context dest create namespace guestbook kubectl --context dest --namespace guestbook --recursive=true apply -f ./output Note: To change the namespace, use Kustomize.\ncd output/resources/guestbook kustomize init --namespace custom-guestbook --autodetect The result is a kustomization.yaml like the example below:\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ConfigMap_guestbook_kube-root-ca.crt.yaml - Deployment_guestbook_frontend.yaml - Deployment_guestbook_redis-master.yaml - Deployment_guestbook_redis-slave.yaml - Secret_guestbook_default-token-5vsrb.yaml - ServiceAccount_guestbook_default.yaml - Service_guestbook_frontend.yaml - Service_guestbook_redis-master.yaml - Service_guestbook_redis-slave.yaml namespace: custom-guestbook After creating the custom-guestbook namespace, apply the kustomization.yaml with kubectl apply -k.\nNext Steps  Read more about Crane. Check out Crane Runner where you can perform application migrations inside Kubernetes.  Cleanup kubectl --context dest delete namespace guestbook "},{"uri":"http://konveyor.github.io/crane/installation/","title":"Installing Crane","tags":[],"description":"","content":"Follow the procedure below to install the Crane tool.\n1. Install the Crane binary. Enter the following command to install the latest version of Crane binary.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /usr/bin/crane Crane currently supports three architectures:\n amd64-linux amd64-darwin arm64-darwin  Run the following command to download the latest version of Crane for amd64-linux.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-linux\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for amd64-darwin.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for arm64-darwin.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;arm64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- 2. Install the most recent version of Crane from the upstream main branch. GOPATHshould be configured to build the project.\ncd $GOPATH git clone https://github.com/konveyor/crane.git cd crane go build -o crane main.go cp crane /usr/bin/crane Note: Install the released version instead of building from upstream main.\n"},{"uri":"http://konveyor.github.io/crane/tutorials/migratek8cluster/","title":"Tutorial: Migrating a Kubernetes cluster","tags":[],"description":"","content":"Migrating a Kubernetes cluster This tutorial follows a demo showing how to use the Konveyor tool Crane to migrate an application (inventory) from the source Kubernetes cluster (src) to the destination cluster (dest).\nRefer to the Crane Documentation for more detailed information.\nNote: In addition to migrating with Crane, it is helpful to push the application to git so it can be automatically deployed to any cluster in the future. This demo includes those steps.\n View details of the destination and source clusters.   \u0026gt; $ minikube profile list View the applications on the source cluster.   \u0026gt; $ kubectl --context src get namespace View the inventory and postgres services.   \u0026gt; $ kubectl --context src --namespace inventory get all View the inventory storage capacity.   \u0026gt; $ kubectl --context src --namespace inventory get pvc View the front-end database content.   \u0026gt; $ curl http://… Set the context to the source.   \u0026gt; $ kubectx src List the application namespaces on the source.   \u0026gt; $ kubectl get ns Tip: Use the kubectx and kubectl get ns commands to change the context to the destination and verify the migrating application does not exist.\nCreate an export folder.   \u0026gt; $crane export --namespace=inventory View what was extracted from the source cluster into yaml files.   \u0026gt; $ tree export Note: These files cannot be imported into another cluster because of the existing IP addresses, timestamps, etc. which should not be pushed to git. View this data using the cat command.\nWhen the application is migrated, this information must be updated to the new cluster.\nClean the manifest files before pushing to git.   \u0026gt; $ crane transform list-plugins Note: This example uses the Kubernetes plugin. Use the crane plugin-manager list command to view available plugins to install and translate the manifest into OpenShift for example.\nView available options that can be used to change the manifests before pushing to git.   \u0026gt; $ crane transform optionals Create a transform folder using the default Kubernetes transform options.   \u0026gt; $ crane transform View the patches in the transform folder that will be applied to the original manifests to create new ones.   \u0026gt; $ tree transform Open the patches and verify the data that will be changed before the transform.   \u0026gt; $ cat [directory path] Clean up the manifest files using the patches and create an output folder.   \u0026gt; $ crane apply View the cleaned manifest files.   \u0026gt; $ tree output Copy the cleaned manifests to the git folder for future migrations.   \u0026gt; $ crane apply -o Change the context to the destination.   \u0026gt; $ kubectx dest Create a new namespace.   \u0026gt; $ kubectl --context dest create ns inventory \\ namespace/inventory created Verify the name of the pvc needed for migration.   \u0026gt; $ kubectl --context src --namespace inventory get pvc Begin the migration.   \u0026gt; $ crane transfer-pvc --source-context=src \\ --pvc-name=postgres-pv-claim --destination-context=dest Tip: Perform the migration while the application is running and then again during a maintenance window when the application has been shut down to only migrate over the delta between the migrations. 22. List the contents of the demo directory.\n \u0026gt; $ ls View the migrated files.   \u0026gt; $ tree output View the argo inventory file and verify the git repoURL, namespace, and server.   \u0026gt; $ cat inventory.argo.yaml Copy the file into the argo cd.   \u0026gt; $ kubectl --context dest --namespace argocd apply -f inventory.argo.yaml Open the Argo CD user interface. Open the migrated application. Verify Argo picked up and provisioned all the manifests from the git repo. Click the Sync button at the top of the screen where the application can be resynced from the git repo.  "},{"uri":"http://konveyor.github.io/","title":"Konveyor Documentation","tags":[],"description":"","content":"Sysadmins and Developers are tired of the words “Digital Transformation.” They don’t want to hear about the “five keys to digital transformation” or the “seven must-haves to transform.” They are also tired of every vendor presenting their framework and methodology for digital transformation bundled with a bunch of proprietary tools.\nDevelopers and sysadmins want to learn how to actually transform their applications and infrastructure so they can take advantage of new technologies to deliver new capabilities faster, at greater scale, and with higher quality while reducing technical debt. It is clear that one of the technologies that underpins the future of applications and infrastructure is Kubernetes - an open-source system for automating deployment, scaling, and management of containerized applications.\nThe Konveyor community exists to accelerate sysadmins and developers\u0026rsquo; journey to Kubernetes. Konveyor is a community of people passionate about helping others modernize and migrate their applications for Kubernetes by building tools, identifying patterns, and providing advice.\nKonveyor projects Konveyor currently consists of five tools or projects:\nForklift is focused on migrating virtual machines to Kubernetes and provides the ability to migrate multiple virtual machines to KubeVirt with minimal downtime. It allows organizations to rehost, or “lift and shift\u0026rsquo;\u0026rsquo; applications residing on these VMs. While rehosting doesn’t provide the same depth of benefits as replatforming or refactoring, it’s the first step in the right direction. It’s often useful to have all the unmodified development workloads in Kubernetes as a basecamp for further transformations, or in cases where development teams may not have the ability to change or modify code, such as with vendor provided software. Rehosting also helps teams enjoy the benefits of the new platform with less friction in improving process and culture.\nThe downstream version of this tool, Migration Toolkit for Virtualization (MTV) is available to Red Hat customers interested in moving vSphere virtual machines to OpenShift Virtualization. As of February 2021, it is available as tech preview.\nCrane is another rehosting tool that meets a different use case. It allows organizations to migrate applications between Kubernetes clusters. There are many times when developers and operations teams want to migrate between older and newer versions of Kubernetes, evacuate a cluster, or migrate to different underlying infrastructure. In an ideal scenario, this would be a redeployment of the application, but in reality we have found that many users need a solution for migrating persistent data and the objects within Kubernetes namespaces continuously.\nThe downstream version of Crane, Migration Toolkit for Containers (MTC), is available for Red Hat OpenShift customers interested in moving from version 3.x to 4, as well as from different clusters of version 4. It is fully supported and available on OpenShift Operator Hub.\nMove2Kube is a project that allows customers to replatform their applications to Kubernetes orchestrated platforms. Replatforming, or “lift, tinker, and shift”, involves changing an underlying technology used by an application while minimizing the need for code change. One area where replatforming is taking place is in the consolidation of container orchestration platforms to Kubernetes. Due to this consolidation, the Move2Kube project was started to focus on accelerating the process of replatform to Kubernetes from platforms such as Swarm and Cloud Foundry. The project translates existing artifacts to Kubernetes artifacts to speed up the process of being able to run applications on Kubernetes.\nTackle provides a series of interrelated tools that allows users to assess, analyze, and ultimately move their applications onto a Kubernetes orchestrated platform. Often considered the most challenging application modernization strategy, adapting applications to a containerized runtime, also offers the largest potential long term impact. This strategy involves making changes to the application and development to take advantage of cloud native capabilities. The tool helps assess the depth of the changes ranging from minimal fixes to adapt the application to containers to a full rewrite of the application in more modern container-friendly runtimes.The Tackle project provides tools that inventory an application environment and identify which workloads are most suitable for refactoring into containers. A common application inventory can also be generated which is then made available as a basis for code execution. The team that is catalyzing this project has experience in these areas from working on tools such as Pathfinder and Windup and will be bringing these experiences to their work on the Tackle project.\nThe downstream version of Tackle, Migration Toolkit for Applications (MTA), is an assembly of tools that support large-scale Java application modernization and migration projects across a broad range of transformations and use cases. MTA accelerates application code analysis, supports effort estimation, accelerates code migration, and helps users move applications to a variety of platforms including OpenShift.\nPelorus is focused around measuring the improvement that moving and modernizing actually delivers as described in the Accelerate book, the reference in DevOps. The community feels strongly that being able to measure the impact of rehosting, replatform, refactoring, and changing processes and culture is vital to proving value. Pelorus is a project focused on measuring the key metrics to software delivery performance (lead time for change, deployment frequency, mean time to restore, and change failure rate) and enabling metrics driven transformation.\nTogether, these five projects comprise today the Konveyor community. Moving forward, it is inevitable that demand for each will align with evolving migration strategies and technology trends. For example, whereas rehosting with Forklift defined use cases may reach heightened demand in the short term, this project could also very well lose traction as Virtual Machines are fully containerized across the landscape in the longer term. New tools could then hypothetically emerge in the Konveyor community empowering users to further extend these containerized applications to edge computing environments, perhaps relying on AI/ML and event-driven architectures. That being said, it is widely expected that there will be ebb and flow with existing project usage, as well as the introduction of entirely new projects altogether.\n-Content taken from the Konveyor Messaging Guide\n"},{"uri":"http://konveyor.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]