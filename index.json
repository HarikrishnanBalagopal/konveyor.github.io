[{"uri":"http://konveyor.github.io/crane/","title":"Crane","tags":[],"description":"","content":"Crane Use this section to better understand and use the Konveyor Crane tool.\n"},{"uri":"http://konveyor.github.io/forklift/","title":"Forklift","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"http://konveyor.github.io/tackle/","title":"Tackle","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step1export/","title":"Step One: Export","tags":[],"description":"","content":"The first step of the cluster migration process is exporting resources from a source cluster of any namespace to be input for the subsequent commands.\nAll of the following export commands will output the contents of the foo namespace into a local export directory with the context demo defined in KUBECONFIG.\ncrane export -n foo -e export --context demo cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: demo EOF crane export -c conf.yaml cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: testing EOF crane export -c conf.yaml --context demo # Note the difference is we are overriding \u0026#34;context\u0026#34; here with flag Note: There are multiple ways to input a command, precedence of which is input from flags \u0026gt; input from config file \u0026gt; env variables \u0026gt; default values (not all the flags can have a corresponding env variable). This behavior persists across all Crane CLI commands.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step2transform/","title":"Step Two: Transform","tags":[],"description":"","content":"The Transform command facilitates the changes to the exported resources that are frequently necessary when migrating workloads between one environment to another. For example:\n Stripping the resource status information that is no longer relevant after the resource is serialized out of a cluster. Adjusting resource quotas to fit the destination environment. Altering node selectors to match the new environment if the node labels do not match the source environment. Applying custom labels or annotations to resources during the migration.  Because most changes are specific to an environment, Crane is designed for total customization and the transform command accepts a “plugins” directory argument.\nEach plugin is an executable with a well defined stdin/out interface allowing for customization or installation and use of published generic plugins.\nAfter exporting the resources into a local directory and installing the desired transformation plugins, the crane transform command can run. The output is placed in a directory with a set of transform files that describe the changes that need to be applied to the original resources before their final import. The changes are written in the JSONPatch format, are human readable, and easily hackable.\nThis command generates a patch to add an annotation transform-test:test for objects in the export directory and the transform directory to be used as input for apply command.\ncrane transform -e export -p plugins -t transform --optional-flags=\u0026#34;add-annotations=transform-test:test\u0026#34; Run the following to see the list of available optional commands for configured plugins.\ncrane transform optionals See Managing PlugIns for more information on plugins that can be consumed by the transform command.\n"},{"uri":"http://konveyor.github.io/crane/overview/","title":"Overview","tags":[],"description":"","content":"Crane is a tool that helps application owners migrate Kubernetes workloads and their state between clusters, remove environment-specific configuration, and automate application deployments along the way.\nThe process uses a few tools:\n crane: The command line tool that migrates applications to the terminal. crane-lib: The brains behind Crane functionality responsible for transforming resources. crane-plugin-openshift: Plugin specifically tailored to manage OpenShift migration workloads and an example of a repeatable best-practice. crane-plugins: Collection of plugins from the Konveyor community based on experience from performing Kube migrations.  Why crane is needed? Crane is the product of several years of experience performing large-scale production Kubernetes migrations that are large, complex, error-prone, and usually peformed in a limited window of time.\nTo face those challenges, the Crane migration tool is designed with transparency and ease-of-diagnostics in mind. It drives migration through a pipeline of non-destructive tasks that output results to disk so the operation can be easily audited and versioned without ever impacting live workloads. The tasks can be run repeatedly and will output consistent results given the same inputs without side-effects on the system at large.\n"},{"uri":"http://konveyor.github.io/crane/tutorials/","title":"Tutorials","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/crane/tools/","title":"Tools","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/crane/usingcrane/","title":"Using Crane","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/crane/tools/gitopsintegration/","title":"Integrating GitOps","tags":[],"description":"","content":"All Crane commands are individual utilities, but when used together in sequence, they form a pipeline.\nCrane makes it easy to integrate a gitops that applies the patches/resources generated at the end of the apply command on the destination cluster. The resources generated at the end of the process (i.e export, transform, apply) can be pushed to a github repository, and a pipeline can be created to deploy the resources on a cluster on every push.\n"},{"uri":"http://konveyor.github.io/crane/tools/customplugins/","title":"Developing custom plugins","tags":[],"description":"","content":"This document covers how to write a plugin binary using crane-lib. It requires:\n  Go to the development environment setup. (Optionally, an overview of the crane toolkit.)\n  Create binary plugin for crane-lib as a simple Go program in the following format that will:\n   Read an input from stdin. Call the Run function with the input object passed as unstructured. Print the return value of Run function on stdout.  package main import ( \u0026#34;fmt\u0026#34; jsonpatch \u0026#34;github.com/evanphx/json-patch\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform/cli\u0026#34; ) func main() { fields := []transform.OptionalFields{ { FlagName: \u0026#34;my-flag\u0026#34;, Help: \u0026#34;What the flag does\u0026#34;, Example: \u0026#34;true\u0026#34;, }, } cli.RunAndExit(cli.NewCustomPlugin(\u0026#34;MyCustomPlugin\u0026#34;, \u0026#34;v1\u0026#34;, fields, Run)) } func Run(request transform.PluginRequest) (transform.PluginResponse, error) { // plugin writers need to write custom code here. resp := transform.PluginResponse{ Version: string(transform.V1), } // prepare the response return resp, nil } The json is passed in using stdin is a transform.PluginRequest which consists of an inline unstructured object and an optional Extras map containing additional flags. Without any Extras the format is identical to the json output from a kubectl get -o json call.\nWhen adding extra parameters, a map field “extras” is added at the top level (parallel to “apiVersion”, “kind”, etc.).\nVersion the plugin development output by passing in the JSOC object on stdin manually during development. For example, if the code above is compiled and run with the following json as input, the output will be {\u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot;}.\n./my-plugin { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Route\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;openshift.io/host.generated\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:spec\u0026#34;: { \u0026#34;f:path\u0026#34;: {}, \u0026#34;f:to\u0026#34;: { \u0026#34;f:kind\u0026#34;: {}, \u0026#34;f:name\u0026#34;: {}, \u0026#34;f:weight\u0026#34;: {} }, \u0026#34;f:wildcardPolicy\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;oc\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:ingress\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;openshift-router\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;mssql-app-route\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;mssql-persistent\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;155816271\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/route.openshift.io/v1/namespaces/mssql-persistent/routes/mssql-app-route\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;42dca205-31bf-463d-b516-f84064523c2c\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;to\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mssql-app-service\u0026#34;, \u0026#34;weight\u0026#34;: 100 }, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;ingress\u0026#34;: [ { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Admitted\u0026#34; } ], \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerCanonicalHostname\u0026#34;: \u0026#34;apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; } ] } } When the plugin is ready to be tested, put it in a directory and run with the crane cli command.\nMore accurate detail can be found [here] (https://github.com/konveyor/crane-lib/blob/main/transform/binary-plugin/README.md).\n"},{"uri":"http://konveyor.github.io/crane/tools/tunnelapi/","title":"Tunnel API","tags":[],"description":"","content":"The tunnel-api sub-command can be used to access an on-premise cluster from a cloud cluster to allow orchestrating migrations from on-premise clusters using MTC where access is not possible otherwise.\nAn openvpn client on the on-premise cluster will connect to a server running on the cloud cluster and the openvpn server is exposed to the client using a load balancer address on the cloud cluster.\nA service created on the cloud cluster is used to expose the on-premise clusters API to MTC running on the cloud cluster.\nRequirements  The system used to create the VPN tunnel must have access and be logged in to both clusters. It must be possible to create a load balancer on the cloud cluster. An available namespace on each cluster to run the tunnel in not created in advance.  Note: To connect multiple on-premise source clusters to your cloud cluster use a separate namespace for each.\napi-tunnel options  namespace: The namespace used to launch the VPN tunnel in, defaults to openvpn destination-context: The cloud destination cluster context where the openvpn server will be launched. destination-image: The container image to use on the destination cluster. (Default: quay.io/konveyor/openvpn:latest) source-context: The on-premise source cluster context where the openvpn client will be launched. source-image: The container image to use on the source cluster. (Default: quay.io/konveyor/openvpn:latest) proxy-host: The hostname of an http-proxy to use on the source cluster for connecting to the destination cluster. proxy-pass: The password for the http-proxy. If specified you must also specify a username or it will be ignored. proxy-port: The port the http-proxy is listening on. If none is specified it will default to 3128 proxy-user: The username for the http-proxy. If specified you must also specify a password or it will be ignored.  Example\ncrane tunnel-api --namespace openvpn-311 \\ --destination-context openshift-migration/c131-e-us-east-containers-cloud-ibm-com/admin \\ --source-context default/192-168-122-171-nip-io:8443/admin \\ --source-image: my.registry.server:5000/konveyor/openvpn:latest \\ --proxy-host my.proxy.server \\ --proxy-port 3128 \\ --proxy-user foo \\ --proxy-pass bar MTC Configuration When configuring the source cluster in MTC the API URL takes the form of https://proxied-cluster.${namespace}.svc.cluster.local:8443.\nYou may also set the image registry for direct image migrations to proxied-cluster.${namespace}.svc.cluster.local:5000.\nReplace ${namespace} with either openvpn or the namespace you specified when running the command to set up the tunnel.\nDemo https://youtu.be/wrPVcZ4bP1M\nTroubleshooting It may take 3 to 5 minutes after the setup to complete for the load balancer address to become resolvable. During this time the client will be unable to connect and establish a connection and the tunnel will not function.\nDuring this time you can run oc get pods in the namespace you specified for setup, and monitor the logs of the openvpn container to see the connection establish.\nExample\noc logs -f -n openvpn-311 openvpn-7b66f65d48-79dbs -c openvpn "},{"uri":"http://konveyor.github.io/crane/tools/pluginmanager/","title":"Plugin Manager","tags":[],"description":"","content":"The Plugin Manager is an optional utility that assists in adding plugins to the appropriate location to be consumed by the transform command.\nList Plugin utility The List Plugin utility discovers available plugins that that are compatible with the current OS and architecture.\ncrane plugin-manager list Listing from the repo default +-----------------+------------------+-------------------+ | NAME | SHORTDESCRIPTION | AVAILABLEVERSIONS | +-----------------+------------------+-------------------+ | OpenshiftPlugin | OpenshiftPlugin | v0.0.1 | +-----------------+------------------+-------------------+ Other valid execution examples This command lists all installed plugins managed by plugin-manager.\ncrane plugin-manager --installed -p plugin-dir This command lists all version of the foo plugin with detailed information.\ncrane plugin-manager --params -n foo Add Plugin utility The Add Plugin utility places the plugin into a directory to be consumed by Transform command.\nThis command downloads the binary of the foo version 0.0.1 plugin from the appropriate source and places it in the plugin-dir/managed directory (the default is plugins).\ncrane plugin-manager add foo --version 0.0.1 -p plugin-dir Remove Plugin utility The Remove Plugin utility removes unwanted plugins from being consumed by the Transform command. This command removes the foo plugin from the plugin-dir/managed dir.\ncrane plugin-manager remove foo -p plugin-dir Note: The plugin-manager command operates in the \u0026lt;plugin-dir\u0026gt;/managed directory. Whenever the flag -p, plugin-dir is used with plugin-manager, the utility operates in the managed places folder in \u0026lt;plugin-dir\u0026gt;.\nFor example: plugin-manager add places the plugin binary within \u0026lt;plugin-dir\u0026gt;/managed, plugin-manager removes the binary from \u0026lt;plugin-dir\u0026gt;/managed, and plugin-manager list --installed uses the path \u0026lt;plugin-dir\u0026gt;/managed to list installed plugins.\nManual plugin management Currently only two plugins are available with more plugins available soon.\nAvailable plugins: -Kubernetes (build into crane-lib) -OpenShift.\nThese plugins can be added to the desired plugin directory. (The default directory is plugin where crane is installed.)\nImportant: The Kubernetes plugin is built into the crane-lib and is not to be added manually or otherwise.\nTo install the plugins:\nDownload the binary of the plugin from the release and place it in the plugin directory.\ncurl -sL https://api.github.com/repos/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Build the binary locally and place it in theplugin directory.\ncd $GOPATH git clone https://github.com/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;.git cd crane-plugin-\u0026lt;plugin-name\u0026gt; go build -f \u0026lt;plugin-name\u0026gt; . cp \u0026lt;plugin\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Note: Adding plugins available in the plugin repo manually is not advisable as long as it can be added usingplugin-manager. For custom plugins or testing plugins under development, manual management is necessary.\n"},{"uri":"http://konveyor.github.io/crane/tutorials/statelessappmirror/","title":"Tutorial: Stateless application mirror","tags":[],"description":"","content":"This tutorial demonstrates how to mirror a simple, stateless PHP Guestbook application using Crane.\nRoadmap\n 1. Deploy the Guestbook application in the source cluster. 2. Extract resources from the source cluster using Crane Export. 3. Transform resources to prepare manifests for the destination cluster using Crane Transform. 4. Apply the transformations using Crane Apply.  Apply application manifests to the destination cluster.    Prerequisites  Create a source and destination Kubernetes cluster environment in minikube or Kind:  minikube\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane/main/hack/minikube-clusters-start.sh\u0026#34; chmod +x minikube-clusters-start.sh./minikube-clusters-start.sh Kind\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane-runner/main/hack/kind-up.sh\u0026#34; chmod +x kind-up.sh./kind-up.sh Install Crane using the Installation Guide.  Important: Read through Kubernetes\u0026rsquo; documentation on accessing multiple clusters. This document references src and dest contexts that refer to the clusters created using the minikube startup scripts above.\nIf you are working in your own environment, or use kind (kind-src and kind-dest), you will need to modify the commands below to reference the correct cluster context.\n1. Deploy the Guestbook application in the source cluster Deploy the Kubernetes\u0026rsquo; stateless guestbook application and modify it to be consumable with Kustomize (Kubernetes native and template-free tool to manage application configuration). The guestbook application consists of:\n redis leader deployment and service redis follower deployment and service guestbook front-end deployment and service  kubectl --context src create namespace guestbook kubectl --context src --namespace guestbook apply -k github.com/konveyor/crane-runner/examples/resources/guestbook kubectl --context src --namespace guestbook wait --for=condition=ready pod --selector=app=guestbook --timeout=180s Optional\nForward localhost traffic to the frontend of the Guestbook application to access Guestbook from the browser of your choice using localhost:8080:\nkubectl --context src --namespace guestbook port-forward svc/frontend 8080:80 2. Extract from the source cluster Crane’s export command is how you extract all of the resources you want from the “source” cluster.\ncrane export --context src --namespace guestbook Check the export directory to verify it is working correctly. The directory should look similar to the example below:\n$ tree -a export export ├── failures │ └── guestbook └── resources └── guestbook ├── ConfigMap_guestbook_kube-root-ca.crt.yaml ├── Deployment_guestbook_frontend.yaml ├── Deployment_guestbook_redis-master.yaml ├── Deployment_guestbook_redis-slave.yaml ├── Endpoints_guestbook_frontend.yaml ├── Endpoints_guestbook_redis-master.yaml ├── Endpoints_guestbook_redis-slave.yaml ├── EndpointSlice_guestbook_frontend-bkqbs.yaml ├── EndpointSlice_guestbook_redis-master-hxr5k.yaml ├── EndpointSlice_guestbook_redis-slave-8wt7z.yaml ├── Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml ├── Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml ├── Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml ├── Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml ├── Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml ├── Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml ├── ReplicaSet_guestbook_frontend-5fd859dcf6.yaml ├── ReplicaSet_guestbook_redis-master-55d9747c6c.yaml ├── ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml ├── Secret_guestbook_default-token-5vsrb.yaml ├── ServiceAccount_guestbook_default.yaml ├── Service_guestbook_frontend.yaml ├── Service_guestbook_redis-master.yaml └── Service_guestbook_redis-slave.yaml 4 directories, 24 files Crane Export is using a discovery client to see all of the API resources in the specified namespace of the designated cluster and outputing them to the disk in YAML form. This allows you to migrate your workloads in a non-destructive way.\nGoing forward you’ll be working against these manifests on the disk without impacting the active resources in the “source” cluster.\n3. Generate Transformations Crane’s transform command generates tranformations in the form of JSON patches and stores them on the disk in the transform directory (unless overridden using ``\u0026ndash;transform-dir`).\ncrane transform Check the transform directory to verify the command worked correctly:\n$ tree -a transform The directory should look similar to the example below:\ntransform └── resources └── guestbook ├── transform-ConfigMap_guestbook_kube-root-ca.crt.yaml ├── transform-Deployment_guestbook_frontend.yaml ├── transform-Deployment_guestbook_redis-master.yaml ├── transform-Deployment_guestbook_redis-slave.yaml ├── transform-Secret_guestbook_default-token-5vsrb.yaml ├── transform-ServiceAccount_guestbook_default.yaml ├── transform-Service_guestbook_frontend.yaml ├── transform-Service_guestbook_redis-master.yaml ├── transform-Service_guestbook_redis-slave.yaml ├── .wh.Endpoints_guestbook_frontend.yaml ├── .wh.Endpoints_guestbook_redis-master.yaml ├── .wh.Endpoints_guestbook_redis-slave.yaml ├── .wh.EndpointSlice_guestbook_frontend-bkqbs.yaml ├── .wh.EndpointSlice_guestbook_redis-master-hxr5k.yaml ├── .wh.EndpointSlice_guestbook_redis-slave-8wt7z.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml ├── .wh.Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml ├── .wh.Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml ├── .wh.Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml ├── .wh.ReplicaSet_guestbook_frontend-5fd859dcf6.yaml ├── .wh.ReplicaSet_guestbook_redis-master-55d9747c6c.yaml └── .wh.ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml 2 directories, 24 files Crane Transform is iterating through the configured plugins and running them against the exported resources from the previous step. You can see which plugins are configured with Crane Transform list-plugins and optional arguments to those plugins with crane transform optionals.\nExplore what plugins can be configured with the Crane plugin-manager list, install one, and customize the exported resources:\ncrane transform --optional-flags=\u0026#34;add-annotations=custom-crane-annotation=foo\u0026#34; If the flags get difficult to manage via the command-line, specify a --flags-file similar to the example below::\ndebug: false export-dir: myExport transform-dir: myTransform output-dir: myOutput optional-flags: add-annotations: custom-crane-annotation: \u0026#34;foo\u0026#34; 4. Apply Transformations The Crane Apply command takes the exported resources and transformations and renders the results as YAML files that can be applied to another cluster.\ncrane apply Look at one of the transformations created in the last step to better understand the Apply command.\n$ cat transform/resources/guestbook/transform-Deployment_guestbook_frontend.yaml [{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/uid\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/resourceVersion\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/creationTimestamp\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/generation\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/status\u0026#34;}]% When crane applies the transformations for the Guestbook frontend it executes a handful of JSON patches that:\n Remove the UID Remove the resourceVersion Remove the creationTimestamp Remove the generation field Remove the status  The leftover data from the source cluster is removed from the final manifests to make them applicable to the destination cluster.\nThe resources are effectively cluster agnostic and ready to be kubectl applied to the cluster of your choosing or placed under version control to be later managed by GitOps and CI/CD pipelines.\nNote: Additional patches to add/remove/replace additional fields on the resources previously exported are available if optional flags are specified..\nApply the manifests to the destination cluster Apply the manifests prepared for the destination cluster using kubectl directly:\nkubectl --context dest create namespace guestbook kubectl --context dest --namespace guestbook --recursive=true apply -f ./output Note: To change the namespace, use Kustomize.\ncd output/resources/guestbook kustomize init --namespace custom-guestbook --autodetect The result is a kustomization.yaml like the example below:\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ConfigMap_guestbook_kube-root-ca.crt.yaml - Deployment_guestbook_frontend.yaml - Deployment_guestbook_redis-master.yaml - Deployment_guestbook_redis-slave.yaml - Secret_guestbook_default-token-5vsrb.yaml - ServiceAccount_guestbook_default.yaml - Service_guestbook_frontend.yaml - Service_guestbook_redis-master.yaml - Service_guestbook_redis-slave.yaml namespace: custom-guestbook After creating the custom-guestbook namespace, apply the kustomization.yaml with kubectl apply -k.\nNext Steps  Read more about Crane. Check out Crane Runner where you can perform application migrations inside Kubernetes.  Cleanup kubectl --context dest delete namespace guestbook "},{"uri":"http://konveyor.github.io/crane/installation/","title":"Installing Crane","tags":[],"description":"","content":"Follow the procedure below to install the Crane tool.\n1. Install the Crane binary. Enter the following command to install the latest version of Crane binary.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /usr/bin/crane Crane currently supports three architectures:\n amd64-linux amd64-darwin arm64-darwin  Run the following command to download the latest version of Crane for amd64-linux.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-linux\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for amd64-darwin.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for arm64-darwin.\ncurl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;arm64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- 2. Install the most recent version of Crane from the upstream main branch. GOPATHshould be configured to build the project.\ncd $GOPATH git clone https://github.com/konveyor/crane.git cd crane go build -o crane main.go cp crane /usr/bin/crane Note: Install the released version instead of building from upstream main.\n"},{"uri":"http://konveyor.github.io/crane/tutorials/migratek8cluster/","title":"Tutorial: Migrating a Kubernetes cluster","tags":[],"description":"","content":"Migrating a Kubernetes cluster This tutorial follows a demo showing how to use the Konveyor tool Crane to migrate an application (inventory) from the source Kubernetes cluster (src) to the destination cluster (dest).\nRefer to the Crane Documentation for more detailed information.\nNote: In addition to migrating with Crane, it is helpful to push the application to git so it can be automatically deployed to any cluster in the future. This demo includes those steps.\n View details of the destination and source clusters.   \u0026gt; $ minikube profile list View the applications on the source cluster.   \u0026gt; $ kubectl --context src get namespace View the inventory and postgres services.   \u0026gt; $ kubectl --context src --namespace inventory get all View the inventory storage capacity.   \u0026gt; $ kubectl --context src --namespace inventory get pvc View the front-end database content.   \u0026gt; $ curl http://… Set the context to the source.   \u0026gt; $ kubectx src List the application namespaces on the source.   \u0026gt; $ kubectl get ns Tip: Use the kubectx and kubectl get ns commands to change the context to the destination and verify the migrating application does not exist.\nCreate an export folder.   \u0026gt; $crane export --namespace=inventory View what was extracted from the source cluster into yaml files.   \u0026gt; $ tree export Note: These files cannot be imported into another cluster because of the existing IP addresses, timestamps, etc. which should not be pushed to git. View this data using the cat command.\nWhen the application is migrated, this information must be updated to the new cluster.\nClean the manifest files before pushing to git.   \u0026gt; $ crane transform list-plugins Note: This example uses the Kubernetes plugin. Use the crane plugin-manager list command to view available plugins to install and translate the manifest into OpenShift for example.\nView available options that can be used to change the manifests before pushing to git.   \u0026gt; $ crane transform optionals Create a transform folder using the default Kubernetes transform options.   \u0026gt; $ crane transform View the patches in the transform folder that will be applied to the original manifests to create new ones.   \u0026gt; $ tree transform Open the patches and verify the data that will be changed before the transform.   \u0026gt; $ cat [directory path] Clean up the manifest files using the patches and create an output folder.   \u0026gt; $ crane apply View the cleaned manifest files.   \u0026gt; $ tree output Copy the cleaned manifests to the git folder for future migrations.   \u0026gt; $ crane apply -o Change the context to the destination.   \u0026gt; $ kubectx dest Create a new namespace.   \u0026gt; $ kubectl --context dest create ns inventory \\ namespace/inventory created Verify the name of the pvc needed for migration.   \u0026gt; $ kubectl --context src --namespace inventory get pvc Begin the migration.   \u0026gt; $ crane transfer-pvc --source-context=src \\ --pvc-name=postgres-pv-claim --destination-context=dest Tip: Perform the migration while the application is running and then again during a maintenance window when the application has been shut down to only migrate over the delta between the migrations. 22. List the contents of the demo directory.\n \u0026gt; $ ls View the migrated files.   \u0026gt; $ tree output View the argo inventory file and verify the git repoURL, namespace, and server.   \u0026gt; $ cat inventory.argo.yaml Copy the file into the argo cd.   \u0026gt; $ kubectl --context dest --namespace argocd apply -f inventory.argo.yaml Open the Argo CD user interface. Open the migrated application. Verify Argo picked up and provisioned all the manifests from the git repo. Click the Sync button at the top of the screen where the application can be resynced from the git repo.  "},{"uri":"http://konveyor.github.io/","title":"Konveyor Documentation","tags":[],"description":"","content":"Konveyor Documentation Content\n"},{"uri":"http://konveyor.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]